---
title: 'Development'
description: 'Advanced Lexa development - Fine-tuning, custom models, and production deployment'
---

<Info>
  **Prerequisites**:
  - Lexa API access and API key
  - Basic understanding of machine learning concepts
  - Python 3.8+ or Node.js 18+ for development
</Info>

Learn how to customize and extend Lexa for your specific use cases and production environments.

## Fine-tuning Lexa

Customize Lexa's behavior for your domain-specific requirements.

<Steps>
<Step title="Prepare your training data">

Lexa fine-tuning requires high-quality training data in a specific format:

```json
[
  {
    "messages": [
      {"role": "user", "content": "What is the capital of France?"},
      {"role": "assistant", "content": "The capital of France is Paris."}
    ]
  },
  {
    "messages": [
      {"role": "user", "content": "How do I make a cake?"},
      {"role": "assistant", "content": "To make a basic cake, you'll need flour, eggs, sugar, and butter..."}
    ]
  }
]
```

<Tip>
Ensure your training data is diverse, high-quality, and follows your desired response patterns.
</Tip>

</Step>

<Step title="Upload your dataset">

Use the Lexa API to upload your training dataset:

```bash
curl -X POST https://api.lexa.chat/v1/files \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -F "file=@training_data.jsonl" \
  -F "purpose=fine-tune"
```

</Step>

<Step title="Create a fine-tuning job">

Start the fine-tuning process with your dataset:

```bash
curl -X POST https://api.lexa.chat/v1/fine_tuning/jobs \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "training_file": "file-abc123",
    "model": "lexa-1",
    "hyperparameters": {
      "n_epochs": 3
    }
  }'
```

</Step>

<Step title="Monitor and deploy">

Track your fine-tuning progress and deploy when complete:

```javascript
// Check fine-tuning status
const job = await lexa.fineTuning.jobs.retrieve('ft-abc123');
console.log(`Status: ${job.status}`);

// Use your fine-tuned model
const response = await lexa.chat.completions.create({
  model: 'ft:lexa-1:your-org:custom-model:1abc123',
  messages: [
    { role: 'user', content: 'Your custom prompt here' }
  ]
});
```

</Step>
</Steps>

## Advanced API Usage

Optimize your Lexa integration with advanced features and best practices.

### Streaming Responses

For real-time applications, use streaming to get responses as they're generated:

```javascript
const stream = await lexa.chat.completions.create({
  model: 'lexa-1',
  messages: [{ role: 'user', content: 'Tell me a story' }],
  stream: true
});

for await (const chunk of stream) {
  const content = chunk.choices[0]?.delta?.content;
  if (content) {
    process.stdout.write(content);
  }
}
```

### Function Calling

Enable structured outputs with function calling:

```javascript
const response = await lexa.chat.completions.create({
  model: 'lexa-1',
  messages: [
    { role: 'user', content: 'What\'s the weather like in San Francisco?' }
  ],
  tools: [
    {
      type: 'function',
      function: {
        name: 'get_weather',
        description: 'Get current weather for a location',
        parameters: {
          type: 'object',
          properties: {
            location: {
              type: 'string',
              description: 'City name'
            }
          },
          required: ['location']
        }
      }
    }
  ]
});
```

### Context Management

Optimize token usage with smart context management:

```javascript
// Track conversation context
let conversation = [
  { role: 'system', content: 'You are a helpful assistant.' }
];

// Add user message
conversation.push({ role: 'user', content: userInput });

// Truncate if context is too long
if (conversation.length > 10) {
  conversation = [
    conversation[0], // Keep system message
    ...conversation.slice(-9) // Keep last 9 messages
  ];
}

const response = await lexa.chat.completions.create({
  model: 'lexa-1',
  messages: conversation,
  max_tokens: 1000
});

// Add assistant response to context
conversation.push(response.choices[0].message);
```

## Production Deployment

Deploy Lexa-powered applications with confidence.

### Rate Limiting and Caching

Implement proper rate limiting and caching strategies:

```javascript
import rateLimit from 'express-rate-limit';
import Redis from 'ioredis';

const redis = new Redis(process.env.REDIS_URL);

// Rate limiting
const limiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 100, // limit each IP to 100 requests per windowMs
  message: 'Too many requests from this IP'
});

// Response caching
async function getCachedResponse(prompt, userId) {
  const cacheKey = `lexa:${userId}:${hash(prompt)}`;
  const cached = await redis.get(cacheKey);
  
  if (cached) {
    return JSON.parse(cached);
  }
  
  const response = await lexa.chat.completions.create({
    model: 'lexa-1',
    messages: [{ role: 'user', content: prompt }]
  });
  
  // Cache for 1 hour
  await redis.setex(cacheKey, 3600, JSON.stringify(response));
  return response;
}
```

### Error Handling

Implement robust error handling for production:

```javascript
async function safeLexaCall(prompt) {
  try {
    const response = await lexa.chat.completions.create({
      model: 'lexa-1',
      messages: [{ role: 'user', content: prompt }],
      timeout: 30000 // 30 second timeout
    });
    
    return {
      success: true,
      data: response.choices[0].message.content
    };
  } catch (error) {
    console.error('Lexa API error:', error);
    
    if (error.code === 'rate_limit_exceeded') {
      return {
        success: false,
        error: 'Rate limit exceeded. Please try again later.'
      };
    }
    
    if (error.code === 'timeout') {
      return {
        success: false,
        error: 'Request timed out. Please try again.'
      };
    }
    
    return {
      success: false,
      error: 'An unexpected error occurred. Please try again.'
    };
  }
}
```

## Performance Optimization

Optimize your Lexa integration for better performance and cost efficiency.

<AccordionGroup>
  <Accordion title="Token Optimization">
    - Use shorter, more specific prompts
    - Implement context truncation strategies
    - Consider using lexa-1-fast for speed-critical applications
    - Cache frequently requested responses
  </Accordion>

  <Accordion title="Cost Management">
    - Monitor token usage with detailed logging
    - Implement usage quotas and alerts
    - Use appropriate models for different use cases
    - Consider fine-tuning for domain-specific efficiency
  </Accordion>

  <Accordion title="Scalability">
    - Use connection pooling for high-traffic applications
    - Implement horizontal scaling with load balancers
    - Consider async processing for non-real-time requests
    - Use CDNs for static content and caching
  </Accordion>
</AccordionGroup>

## Troubleshooting

<AccordionGroup>
  <Accordion title="High latency issues">
    - Check your network connection to Lexa's API endpoints
    - Consider using lexa-1-fast for speed-critical applications
    - Implement request timeouts and retry logic
    - Monitor API response times and optimize prompts
  </Accordion>

  <Accordion title="Token limit exceeded">
    - Truncate conversation history intelligently
    - Use more concise prompts
    - Implement context summarization for long conversations
    - Consider breaking complex requests into smaller chunks
  </Accordion>

  <Accordion title="Fine-tuning failures">
    - Ensure training data quality and format
    - Check dataset size requirements (minimum 10 examples)
    - Verify API key permissions for fine-tuning
    - Monitor job status and error messages
  </Accordion>
</AccordionGroup>

Ready to deploy? Check out our [API reference](/api-reference/introduction) for complete endpoint documentation.
